### Improving the performance of the proposed model

1. Hyperparameter Tuning - 
The implemented model has several hyperparameters, such as `q` in the Generalized Cross Entropy Loss. Increasing `q` makes the model more robust with respect to handling noise in the labels, however, it hampers the feature extraction of the model. The current value of `q` is a guess on my part with there being no reasoning whatsoever behind choosing this particular value, except that `0.7` implies somewhat more robustness than actual feature extraction. Varying `q` as a hyperparameter, indirectly tweaks the loss function as a whole, so it does not make much more sense to try and change the loss function in search of better results.
Varying the standard hyperparameters such as `lr`, `decay`, `momentum`, etc. could yield better results as well.
However, over-tuning the hyperparameters runs the risk of over-fitting the model to the test / validation data.

2. Checkpoints -
There have been times when the test accuracy has reduced between epochs. This could be prevented, and improved upon by using checkpoints in the code, and always using the set of weights which give the best accuracy as the starting point for any epoch, rather than the weights generated by the previous epoch.

3. Framework for clustering and then classifying:
    - Either use K-means or hierarchical clustering on the objects on the basis of their feature vectors
    - Predict the label for a clustered class on the basis of the noisy labels of its constituents.
    - Depending on the level of noise, and the images in the dataset, this method has the potential to give really high accuracy on the test data.
    - The problem with this method is that is there is a very high level of noise in the dataset, then the second part of the framework, which determines the label for an entire class, will not have very high confidence scores.
    - Secondly, if the images present for a particular class in the dataset aren't all of a similar scale or taken from the perspective, and this varies greatly over the set, then there is a very high chance of images getting mis-clustered, which will greatly reduce test accuracy

4. Pre-trained networks
    Using pretrained networks is a very easy and simple way of reducing the effect of noise in the training set. Since a pretrained net already has extracted a lot of features about images, modelling it with the Generalized Cross Entropy Loss can give extremely high accuracy on the test set, while successfully disregarding the noise in the train set.